# Tuning parameters

Back propagation works on many parameters which can be tuned to optimise the
output of a neural network.

Some of the parameters are:

## 1. Learning rate

Learning rate is a constant which determines how rapidly the network proceeds
towerds a minima. Too low learning rate can make the learning process very slow
while higher learning rate can make the network to step over the minima.

## 2. Number of hidden layers

Depends on the complexity of the network,

## 3. Number of nodes in hidden layer

sqrt(No of nodes in input layer \* No. of nodes in output layer)

## 4. Activation function

Choice of activation function in the hidden layer can affect the outcome of the
neural network, ReLU is good for hidden layers as it solves the problem of
vanishing gradient.

## 5. Batch size

## 6. Number of Epochs
